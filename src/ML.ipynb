{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegues/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printWrongPreds(preds, targets):\n",
    "    df = pd.DataFrame()\n",
    "    images = []\n",
    "    predictions = []\n",
    "    targets_ = []\n",
    "    if(type(preds) == pd.DataFrame):          \n",
    "        predsClass = preds.idxmax(axis=1)\n",
    "        targetsClass = targets.idxmax(axis=1)  \n",
    "        for i in range(0,len(predsClass)):\n",
    "            if(predsClass[i] != targetsClass[i]):\n",
    "                images.append(targets.index[i])\n",
    "                predictions.append(predsClass[i])\n",
    "                targets_.append(targetsClass[i])\n",
    "                print('Image:', targets.index[i],'\\t\\tPrediction:', predsClass[i], '\\tTarget:', targetsClass[i])\n",
    "    else:\n",
    "        for i in range(0,len(preds)):\n",
    "            if(preds[i] != targets['level3'][i]):\n",
    "                images.append(targets.index[i])\n",
    "                predictions.append(preds[i])\n",
    "                targets_.append(targets['level3'][i])\n",
    "                print('Image:', targets.index[i],'\\t\\tPrediction:', preds[i], '\\tTarget:', targets['level3'][i])\n",
    "    df['image'] = images\n",
    "    df['prediction'] = predictions\n",
    "    df['target'] = targets_\n",
    "    return df\n",
    "\n",
    "def createDirectoryEnvironment(train_data, train_targets, val_data, val_targets, path):\n",
    "    trainpath = path + 'train/'\n",
    "    valpath = path + 'validation/'\n",
    "    if(os.listdir() == 2):\n",
    "        return trainpath,valpath\n",
    "    if(not os.path.exists(trainpath)):\n",
    "        os.mkdir(trainpath)\n",
    "    if(not os.path.exists(valpath)):\n",
    "        os.mkdir(valpath)\n",
    "        \n",
    "    classes = train_targets.columns.append(val_targets.columns).drop_duplicates()  \n",
    "    for c in classes:\n",
    "        train_class = trainpath + c + '/'\n",
    "        val_class = valpath + c + '/'\n",
    "        if(not os.path.exists(train_class)):\n",
    "            os.mkdir(train_class)\n",
    "        if(not os.path.exists(val_class)):\n",
    "            os.mkdir(val_class)\n",
    "            \n",
    "    for f in train_data.index.values:\n",
    "        copyfile(path + '../' + f[:f.find('frame')-1] + '/' + f, trainpath + train_targets.loc[f].idxmax(axis = 1) + '/' + f)\n",
    "    for f in val_data.index.values:\n",
    "        copyfile(path + '../' + f[:f.find('frame')-1] + '/' + f, valpath + val_targets.loc[f].idxmax(axis = 1) + '/' + f)\n",
    "    \n",
    "    return trainpath, valpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  index  level3\n",
      "0  A5.1     178\n",
      "1  A4.1     178\n",
      "2  A3.1     175\n",
      "3  A4.7     174\n",
      "4  A3.7      91\n",
      "5  A5.2      85\n",
      "6  A5.4      34\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/diegues/Desktop/ProcessedImages/\"\n",
    "data = pd.read_csv(folder_path + \"sampled_data.csv\")\n",
    "#classes = open(folder_path + \"classes.txt\", \"r\").readlines()\n",
    "\n",
    "filenames = data['filename']\n",
    "targets = data['level3']\n",
    "\n",
    "# one-hot encoding\n",
    "targets_ohe = pd.get_dummies(data['level3'])\n",
    "#species_ohe = pd.get_dummies(data['species'])\n",
    "\n",
    "# dealing with NaNs\n",
    "data = data.drop(['roll', 'pitch', 'level1', 'level2', 'level3', 'level4', \n",
    "                  'level5', 'level6', 'AphiaID', 'EunisName', 'EunisCode', \n",
    "                  'date', 'timestamp', 'species'],\n",
    "                 axis = 1)\n",
    "\n",
    "X = data.groupby('filename').max()\n",
    "Y_ohe = pd.concat([filenames,targets_ohe], axis = 1).groupby('filename').max()\n",
    "Y_cat = pd.concat([filenames,targets], axis = 1).groupby('filename').max()\n",
    "\n",
    "print(pd.value_counts(Y_cat.level3).to_frame().reset_index())\n",
    "\n",
    "# tts\n",
    "train_X_ohe, test_X_ohe, train_Y_ohe, test_Y_ohe = train_test_split(X, Y_ohe, test_size = 0.3)\n",
    "train_X_cat, test_X_cat, train_Y_cat, test_Y_cat = train_test_split(X, Y_cat, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:\t 0.9490909090909091\n",
      "Image: 132143_forcadinho-np3_frame2809.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame380.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame354.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame4304.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1129.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame481.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame304.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame260.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 104728_cam-np3_frame62.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame2784.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame4062.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame273.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1394.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 104728_cam-np3_frame68.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "           0      1\n",
      "0   latitude  0.285\n",
      "1  longitude  0.334\n",
      "2    entropy  0.051\n",
      "3      depth  0.330\n"
     ]
    }
   ],
   "source": [
    "# Random Forest prediction\n",
    "rf = RandomForestClassifier(n_estimators = 1000)\n",
    "rf.fit(train_X_ohe, train_Y_ohe)\n",
    "predictions_rf = rf.predict(test_X_ohe)\n",
    "predictions_rf = pd.DataFrame(predictions_rf)\n",
    "predictions_rf.columns = test_Y_ohe.columns.values\n",
    "print('RF:\\t',rf.score(test_X_ohe, test_Y_ohe))\n",
    "failed_rf = printWrongPreds(predictions_rf, test_Y_ohe)\n",
    "print(pd.DataFrame([(name,round(value,3)) for name,value in zip(X.columns,rf.feature_importances_)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\t 0.6290909090909091\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svm = svm.SVC()\n",
    "svm.fit(train_X_cat, train_Y_cat.values.ravel())\n",
    "preds_svm = svm.predict(test_X_cat)\n",
    "print('SVM:\\t',accuracy_score(test_Y_cat, preds_svm))\n",
    "#failed_svm = printWrongPreds(preds_svm, test_Y_cat) # too many wrong predictions to print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn NN:\t 0.8981818181818182\n",
      "Image: 132143_forcadinho-np3_frame4215.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame979.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame881.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame789.jpg \t\tPrediction: A5.2 \tTarget: A4.1\n",
      "Image: 125355_forcadinho-np3_frame1813.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame3012.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame1394.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame4238.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame1598.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame489.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame67.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2969.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame360.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame1138.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame404.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame4068.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame1354.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame1638.jpg \t\tPrediction: A5.2 \tTarget: A5.4\n",
      "Image: 105317_cam-np3_frame970.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame4227.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame4106.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame874.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2265.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame782.jpg \t\tPrediction: A5.2 \tTarget: A4.1\n",
      "Image: 125355_forcadinho-np3_frame1386.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame2917.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame1812.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame302.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks\n",
    "## sklearn\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X_cat)\n",
    "\n",
    "train_X_scaled = scaler.transform(train_X_cat)\n",
    "test_X_scaled = scaler.transform(test_X_cat)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4096,4096,1000))\n",
    "mlp.fit(train_X_scaled,train_Y_cat.values.ravel())\n",
    "\n",
    "predictions_nn = mlp.predict(test_X_scaled)\n",
    "print('sklearn NN:\\t',accuracy_score(test_Y_cat,predictions_nn))\n",
    "failed_nn = printWrongPreds(predictions_nn, test_Y_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 275 samples\n",
      "Epoch 1/50\n",
      "640/640 [==============================] - 8s 12ms/step - loss: 0.4954 - acc: 0.8185 - val_loss: 0.4131 - val_acc: 0.8571\n",
      "Epoch 2/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4352 - acc: 0.8484 - val_loss: 0.4127 - val_acc: 0.8571\n",
      "Epoch 3/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4208 - acc: 0.8554 - val_loss: 0.4073 - val_acc: 0.8571\n",
      "Epoch 4/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4178 - acc: 0.8574 - val_loss: 0.4035 - val_acc: 0.8571\n",
      "Epoch 5/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4062 - acc: 0.8565 - val_loss: 0.4008 - val_acc: 0.8571\n",
      "Epoch 6/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4042 - acc: 0.8569 - val_loss: 0.3962 - val_acc: 0.8571\n",
      "Epoch 7/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4054 - acc: 0.8563 - val_loss: 0.3988 - val_acc: 0.8571\n",
      "Epoch 8/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4017 - acc: 0.8560 - val_loss: 0.3964 - val_acc: 0.8577\n",
      "Epoch 9/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4027 - acc: 0.8567 - val_loss: 0.3951 - val_acc: 0.8571\n",
      "Epoch 10/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4008 - acc: 0.8574 - val_loss: 0.3956 - val_acc: 0.8571\n",
      "Epoch 11/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3991 - acc: 0.8569 - val_loss: 0.3965 - val_acc: 0.8571\n",
      "Epoch 12/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4026 - acc: 0.8567 - val_loss: 0.3984 - val_acc: 0.8571\n",
      "Epoch 13/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4008 - acc: 0.8571 - val_loss: 0.3974 - val_acc: 0.8571\n",
      "Epoch 14/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.4009 - acc: 0.8576 - val_loss: 0.3994 - val_acc: 0.8571\n",
      "Epoch 15/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3952 - acc: 0.8569 - val_loss: 0.3951 - val_acc: 0.8571\n",
      "Epoch 16/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3950 - acc: 0.8569 - val_loss: 0.3969 - val_acc: 0.8571\n",
      "Epoch 17/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3992 - acc: 0.8571 - val_loss: 0.3961 - val_acc: 0.8571\n",
      "Epoch 18/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3984 - acc: 0.8569 - val_loss: 0.3956 - val_acc: 0.8571\n",
      "Epoch 19/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3975 - acc: 0.8569 - val_loss: 0.3999 - val_acc: 0.8571\n",
      "Epoch 20/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3937 - acc: 0.8567 - val_loss: 0.3974 - val_acc: 0.8571\n",
      "Epoch 21/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3941 - acc: 0.8574 - val_loss: 0.3989 - val_acc: 0.8571\n",
      "Epoch 22/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3981 - acc: 0.8569 - val_loss: 0.3966 - val_acc: 0.8571\n",
      "Epoch 23/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3950 - acc: 0.8574 - val_loss: 0.3967 - val_acc: 0.8571\n",
      "Epoch 24/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3967 - acc: 0.8571 - val_loss: 0.3948 - val_acc: 0.8571\n",
      "Epoch 25/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3927 - acc: 0.8574 - val_loss: 0.3971 - val_acc: 0.8577\n",
      "Epoch 26/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3951 - acc: 0.8565 - val_loss: 0.3980 - val_acc: 0.8571\n",
      "Epoch 27/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3951 - acc: 0.8565 - val_loss: 0.3970 - val_acc: 0.8571\n",
      "Epoch 28/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3924 - acc: 0.8569 - val_loss: 0.3969 - val_acc: 0.8571\n",
      "Epoch 29/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3918 - acc: 0.8560 - val_loss: 0.3966 - val_acc: 0.8571\n",
      "Epoch 30/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3935 - acc: 0.8571 - val_loss: 0.3965 - val_acc: 0.8571\n",
      "Epoch 31/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3917 - acc: 0.8571 - val_loss: 0.4010 - val_acc: 0.8561\n",
      "Epoch 32/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3957 - acc: 0.8569 - val_loss: 0.3962 - val_acc: 0.8571\n",
      "Epoch 33/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3857 - acc: 0.8571 - val_loss: 0.3986 - val_acc: 0.8571\n",
      "Epoch 34/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3906 - acc: 0.8571 - val_loss: 0.3994 - val_acc: 0.8571\n",
      "Epoch 35/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3906 - acc: 0.8562 - val_loss: 0.3988 - val_acc: 0.8571\n",
      "Epoch 36/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3926 - acc: 0.8562 - val_loss: 0.4005 - val_acc: 0.8556\n",
      "Epoch 37/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3910 - acc: 0.8567 - val_loss: 0.3964 - val_acc: 0.8571\n",
      "Epoch 38/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3901 - acc: 0.8569 - val_loss: 0.3996 - val_acc: 0.8561\n",
      "Epoch 39/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3897 - acc: 0.8569 - val_loss: 0.4001 - val_acc: 0.8561\n",
      "Epoch 40/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3884 - acc: 0.8569 - val_loss: 0.4011 - val_acc: 0.8556\n",
      "Epoch 41/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3864 - acc: 0.8569 - val_loss: 0.4009 - val_acc: 0.8571\n",
      "Epoch 42/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3891 - acc: 0.8560 - val_loss: 0.3979 - val_acc: 0.8577\n",
      "Epoch 43/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3902 - acc: 0.8569 - val_loss: 0.4018 - val_acc: 0.8566\n",
      "Epoch 44/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3905 - acc: 0.8563 - val_loss: 0.4002 - val_acc: 0.8577\n",
      "Epoch 45/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3904 - acc: 0.8565 - val_loss: 0.4013 - val_acc: 0.8571\n",
      "Epoch 46/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3898 - acc: 0.8569 - val_loss: 0.4004 - val_acc: 0.8566\n",
      "Epoch 47/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3853 - acc: 0.8578 - val_loss: 0.4022 - val_acc: 0.8556\n",
      "Epoch 48/50\n",
      "640/640 [==============================] - 7s 12ms/step - loss: 0.3859 - acc: 0.8574 - val_loss: 0.4040 - val_acc: 0.8571\n",
      "Epoch 49/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3863 - acc: 0.8574 - val_loss: 0.4024 - val_acc: 0.8556\n",
      "Epoch 50/50\n",
      "640/640 [==============================] - 7s 11ms/step - loss: 0.3835 - acc: 0.8571 - val_loss: 0.4061 - val_acc: 0.8561\n",
      "275/275 [==============================] - 0s 2ms/step\n",
      "Keras NN:\t [0.4060875317183408, 0.8561038988286799]\n",
      "Image: 105317_cam-np3_frame32.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame996.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame2862.jpg \t\tPrediction: A4.7 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame419.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame74.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame2476.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame404.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame401.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame851.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame1566.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame1099.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame2469.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame1562.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame92.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame139.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1488.jpg \t\tPrediction: A3.7 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame2904.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame1319.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 125355_forcadinho-np3_frame391.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1124.jpg \t\tPrediction: A3.7 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame1607.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame109.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame2553.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 125355_forcadinho-np3_frame1398.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame511.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame121.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1317.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2481.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame704.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame2563.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame2809.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame304.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame1652.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 132143_forcadinho-np3_frame3115.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame110.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame311.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame3909.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame2550.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame1669.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 105317_cam-np3_frame88.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame945.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 125355_forcadinho-np3_frame1813.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame227.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame2704.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame594.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame2520.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame782.jpg \t\tPrediction: A4.7 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame2532.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame1559.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame1503.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame2620.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame122.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame803.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame354.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1655.jpg \t\tPrediction: A3.1 \tTarget: A5.4\n",
      "Image: 113610_cam_survey_1_frame763.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 125355_forcadinho-np3_frame1444.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame662.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame790.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame248.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame975.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame1318.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame78.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame215.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame593.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame4304.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame1215.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame222.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame481.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame304.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame123.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame1875.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame1885.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame118.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame489.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame484.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 125355_forcadinho-np3_frame198.jpg \t\tPrediction: A3.7 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame213.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame2475.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame767.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 113610_cam_survey_1_frame142.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame2522.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame90.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1481.jpg \t\tPrediction: A4.7 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame214.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame79.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame986.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame228.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame1809.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame260.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame3012.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame415.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame1073.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 125355_forcadinho-np3_frame1229.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame38.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame1665.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 105317_cam-np3_frame96.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame476.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame1561.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame220.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1344.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame1683.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 132143_forcadinho-np3_frame3515.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame24.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame1681.jpg \t\tPrediction: A3.7 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame2982.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame100.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame159.jpg \t\tPrediction: A3.7 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame2669.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame4062.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame35.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame235.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame973.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame2882.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2468.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 125355_forcadinho-np3_frame1812.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2906.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame96.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame1188.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame530.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame769.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame1746.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame977.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame253.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame273.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame1421.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame387.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame1560.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame89.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame1612.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 113610_cam_survey_1_frame1413.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame3971.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame1025.jpg \t\tPrediction: A4.7 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2474.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame537.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame15.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame4227.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame2473.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame403.jpg \t\tPrediction: A3.7 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame191.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame741.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame18.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1084.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1659.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 132143_forcadinho-np3_frame3059.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame2918.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 105317_cam-np3_frame1018.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1699.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 132143_forcadinho-np3_frame1751.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame475.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame404.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame106.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1230.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame172.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame1685.jpg \t\tPrediction: A4.1 \tTarget: A5.4\n",
      "Image: 125355_forcadinho-np3_frame1630.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame629.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame1035.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame783.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame4216.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame2013.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2961.jpg \t\tPrediction: A3.1 \tTarget: A3.7\n",
      "Image: 125355_forcadinho-np3_frame1497.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 113610_cam_survey_1_frame1554.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame328.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame2543.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 132143_forcadinho-np3_frame3360.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 113610_cam_survey_1_frame874.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame1715.jpg \t\tPrediction: A4.1 \tTarget: A5.4\n",
      "Image: 113610_cam_survey_1_frame1579.jpg \t\tPrediction: A3.7 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame3932.jpg \t\tPrediction: A4.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame103.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame1679.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 105317_cam-np3_frame195.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame1484.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame1274.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame1557.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 125355_forcadinho-np3_frame1522.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 132143_forcadinho-np3_frame3955.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame1251.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame1756.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame10.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 125355_forcadinho-np3_frame1342.jpg \t\tPrediction: A3.1 \tTarget: A5.2\n",
      "Image: 113610_cam_survey_1_frame13.jpg \t\tPrediction: A4.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame811.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 113610_cam_survey_1_frame111.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame65.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 113610_cam_survey_1_frame488.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame713.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame474.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame2509.jpg \t\tPrediction: A3.7 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame409.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame1359.jpg \t\tPrediction: A3.7 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame122.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame73.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame600.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame129.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame819.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame119.jpg \t\tPrediction: A5.1 \tTarget: A3.7\n",
      "Image: 132143_forcadinho-np3_frame3472.jpg \t\tPrediction: A4.1 \tTarget: A5.1\n",
      "Image: 105317_cam-np3_frame489.jpg \t\tPrediction: A5.1 \tTarget: A4.1\n",
      "Image: 105317_cam-np3_frame464.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame239.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame2014.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 105317_cam-np3_frame463.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 132143_forcadinho-np3_frame1228.jpg \t\tPrediction: A5.1 \tTarget: A3.1\n",
      "Image: 132143_forcadinho-np3_frame32.jpg \t\tPrediction: A4.7 \tTarget: A4.1\n",
      "Image: 132143_forcadinho-np3_frame1668.jpg \t\tPrediction: A5.1 \tTarget: A5.4\n",
      "Image: 132143_forcadinho-np3_frame1394.jpg \t\tPrediction: A3.1 \tTarget: A5.1\n",
      "Image: 104728_cam-np3_frame68.jpg \t\tPrediction: A3.1 \tTarget: A4.1\n",
      "Image: 113610_cam_survey_1_frame1559.jpg \t\tPrediction: A5.1 \tTarget: A5.2\n",
      "Image: 105317_cam-np3_frame28.jpg \t\tPrediction: A5.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame236.jpg \t\tPrediction: A4.1 \tTarget: A4.7\n",
      "Image: 105317_cam-np3_frame972.jpg \t\tPrediction: A3.1 \tTarget: A4.7\n",
      "Image: 113610_cam_survey_1_frame109.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n",
      "Image: 113610_cam_survey_1_frame405.jpg \t\tPrediction: A4.1 \tTarget: A3.7\n"
     ]
    }
   ],
   "source": [
    "## keras\n",
    "\n",
    "scaler_keras = StandardScaler()\n",
    "scaler_keras.fit(train_X_ohe)\n",
    "\n",
    "X_train_scaled = scaler.transform(train_X_ohe)\n",
    "X_test_scaled = scaler.transform(test_X_ohe)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, activation='relu', input_dim=4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(7, activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X_scaled, train_Y_ohe, epochs=20,validation_data=(test_X_scaled,test_Y_ohe))\n",
    "predskeras_nn = pd.DataFrame(model.predict(test_X_scaled))\n",
    "predskeras_nn.columns = test_Y_ohe.columns.values\n",
    "score = model.evaluate(test_X_scaled, test_Y_ohe)\n",
    "\n",
    "print('Keras NN:\\t', score)\n",
    "failed_keras = printWrongPreds(predskeras_nn,test_Y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclasses = np.sort(np.array(Y_cat['EunisCode'].unique()))\\nclass_map = dict((k,v) for (k, v) in zip(classes, [np.float32(i) for i in range(0,len(classes))]))\\npath_to_imgs = '/home/diegues/Desktop/ProcessedImages/LabeledData/'\\n\\nX_train = []\\nX_test = []\\ny_train = []\\ny_test = []\\ni=0\\nfor file in os.listdir(path_to_imgs):\\n    i = i + 1\\n    print(i)\\n    img = cv2.resize(cv2.imread(path_to_imgs + file, 0),(200,150))\\n    xarray_img = np.squeeze(np.array(img).astype(np.float32))\\n    m, v = cv2.PCACompute(xarray_img, mean = None)\\n    array = np.array(v)\\n    flat_array = array.ravel()\\n    if file in train_X_cat.index:\\n        X_train.append(flat_array)\\n        y_train.append(int(class_map[train_Y_cat['EunisCode'].loc[file]]))\\n        \\n    elif file in test_X_cat.index:\\n        X_test.append(flat_array)\\n        y_test.append(int(class_map[test_Y_cat['EunisCode'].loc[file]]))\\n\\nX_train = np.float32(X_train)\\nX_test = np.float32(X_test)\\ny_train = np.float32(y_train)\\ny_test = np.float32(y_test)\\n\\nimg_svm = cv2.ml.SVM_create()\\nimg_svm.setKernel(cv2.ml.SVM_LINEAR)\\nimg_svm.setType(cv2.ml.SVM_C_SVC)\\nimg_svm.setC(2.67)\\nimg_svm.setGamma(5.383)\\nimg_svm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)\\n        \\nresult = img_svm.predict(X_test)[1]\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image Classification\n",
    "\n",
    "## Support Vector Machines\n",
    "\"\"\"\n",
    "classes = np.sort(np.array(Y_cat['EunisCode'].unique()))\n",
    "class_map = dict((k,v) for (k, v) in zip(classes, [np.float32(i) for i in range(0,len(classes))]))\n",
    "path_to_imgs = '/home/diegues/Desktop/ProcessedImages/LabeledData/'\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "i=0\n",
    "for file in os.listdir(path_to_imgs):\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "    img = cv2.resize(cv2.imread(path_to_imgs + file, 0),(200,150))\n",
    "    xarray_img = np.squeeze(np.array(img).astype(np.float32))\n",
    "    m, v = cv2.PCACompute(xarray_img, mean = None)\n",
    "    array = np.array(v)\n",
    "    flat_array = array.ravel()\n",
    "    if file in train_X_cat.index:\n",
    "        X_train.append(flat_array)\n",
    "        y_train.append(int(class_map[train_Y_cat['EunisCode'].loc[file]]))\n",
    "        \n",
    "    elif file in test_X_cat.index:\n",
    "        X_test.append(flat_array)\n",
    "        y_test.append(int(class_map[test_Y_cat['EunisCode'].loc[file]]))\n",
    "\n",
    "X_train = np.float32(X_train)\n",
    "X_test = np.float32(X_test)\n",
    "y_train = np.float32(y_train)\n",
    "y_test = np.float32(y_test)\n",
    "\n",
    "img_svm = cv2.ml.SVM_create()\n",
    "img_svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "img_svm.setType(cv2.ml.SVM_C_SVC)\n",
    "img_svm.setC(2.67)\n",
    "img_svm.setGamma(5.383)\n",
    "img_svm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)\n",
    "        \n",
    "result = img_svm.predict(X_test)[1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 images belonging to 7 classes.\n",
      "Found 275 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "## Convolutional Neural Networks - VGG config D\n",
    "\n",
    "# Preping the data\n",
    "images_path = '/home/diegues/Desktop/ProcessedImages/SampledData/'\n",
    "#train_dir, val_dir = createDirectoryEnvironment(train_X_ohe, train_Y_ohe, test_X_ohe, test_Y_ohe, images_path)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        images_path + 'train',  \n",
    "        target_size=(224, 224), \n",
    "        batch_size=32,\n",
    "        class_mode='categorical') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        images_path + 'validation',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_62 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 41,558,631\n",
      "Trainable params: 41,558,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fit the models\n",
    "\n",
    "\n",
    "vgg16 = Sequential()\n",
    "\n",
    "vgg16.add(Conv2D(64,(3,3),activation='relu', input_shape=(224,224,3), padding='same'))\n",
    "vgg16.add(Conv2D(64,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(MaxPooling2D((2,2), (2,2)))\n",
    "vgg16.add(Dropout(.25))\n",
    "\n",
    "vgg16.add(Conv2D(128,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(128,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(MaxPooling2D((2,2), (2,2)))\n",
    "vgg16.add(Dropout(.5))\n",
    "\n",
    "vgg16.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(MaxPooling2D((2,2), (2,2)))\n",
    "vgg16.add(Dropout(.5))\n",
    "\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(MaxPooling2D((2,2), (2,2)))\n",
    "vgg16.add(Dropout(.5))\n",
    "\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(Conv2D(512,(3,3),activation='relu', padding='same'))\n",
    "vgg16.add(MaxPooling2D((2,2), (2,2)))\n",
    "vgg16.add(Dropout(.5))\n",
    "\n",
    "vgg16.add(Flatten())\n",
    "vgg16.add(Dense(1024, activation='relu'))\n",
    "vgg16.add(Dropout(0.25))\n",
    "vgg16.add(Dense(1024, activation='relu'))\n",
    "vgg16.add(Dropout(0.5))\n",
    "vgg16.add(Dense(100, activation='relu'))\n",
    "vgg16.add(Dropout(0.75))\n",
    "vgg16.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "vgg16.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/20 [>.............................] - ETA: 26:40 - loss: 0.6944 - acc: 0.4554"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-29b99e8e9174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         validation_steps=len(test_X_ohe) // 32)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg16.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_X_ohe) // 32,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(test_X_ohe) // 32)\n",
    "vgg16.save_weights('vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 images belonging to 7 classes.\n",
      "Found 275 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "images_path = '/home/diegues/Desktop/ProcessedImages/SampledData/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        images_path + 'train',  \n",
    "        target_size=(512, 512), \n",
    "        batch_size=32,\n",
    "        class_mode='categorical') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        images_path + 'validation',\n",
    "        target_size=(512, 512),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 512, 512, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 255, 255, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 127, 127, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 127, 127, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 127, 127, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 63, 63, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 63, 63, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 31, 31, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 31, 31, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 6,751,751\n",
      "Trainable params: 6,751,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DeepSense AI NOAA competition approach\n",
    "\n",
    "dsaiNOAA = Sequential()\n",
    "\n",
    "dsaiNOAA.add(Conv2D(32,(3,3),activation='relu', input_shape=(512,512,3), padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.25))\n",
    "\n",
    "dsaiNOAA.add(Conv2D(64,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.25))\n",
    "\n",
    "dsaiNOAA.add(Conv2D(64,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(Conv2D(128,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(Conv2D(128,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.25))\n",
    "\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.25))\n",
    "\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.5))\n",
    "\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(Conv2D(256,(3,3),activation='relu', padding='same'))\n",
    "dsaiNOAA.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "dsaiNOAA.add(Dropout(.5))\n",
    "\n",
    "dsaiNOAA.add(Flatten())\n",
    "dsaiNOAA.add(Dense(256, activation='relu'))\n",
    "dsaiNOAA.add(Dense(64, activation='relu'))\n",
    "dsaiNOAA.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "dsaiNOAA.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "dsaiNOAA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/20 [>.............................] - ETA: 27:48 - loss: 0.6945 - acc: 0.4866"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9ca7cb2bbfe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         validation_steps=len(test_X_ohe) // 32)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdsaiNOAA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dsaiNOAA.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dsaiNOAA.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_X_ohe) // 32,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(test_X_ohe) // 32)\n",
    "dsaiNOAA.save_weights('dsaiNOAA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
